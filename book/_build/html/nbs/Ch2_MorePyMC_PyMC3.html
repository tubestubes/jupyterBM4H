
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chapter 2 &#8212; Jupyter Book of Probabilistic Programming and Bayesian Methods for Hackers</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.c441f2ba0852f4cabcb80105e3a46ae6.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 3" href="Ch3_IntroMCMC_PyMC3.html" />
    <link rel="prev" title="Probabilistic Programming" href="Ch1_Introduction_PyMC3.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Jupyter Book of Probabilistic Programming and Bayesian Methods for Hackers</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Ch1_Introduction_PyMC3.html">
   Probabilistic Programming
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch3_IntroMCMC_PyMC3.html">
   Chapter 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch4_LawOfLargeNumbers_PyMC3.html">
   Chapter 4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch5_LossFunctions_PyMC3.html">
   Chapter 5
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch6_Priors_PyMC3.html">
   Chapter 6
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch7.1_DontOverfit.html">
   Implementation of Salisman’s Don’t Overfit submission
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch7.1_DontOverfit.html#develop-tim-s-model">
   Develop Tim’s model
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nbs/Ch2_MorePyMC_PyMC3.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/nbs/Ch2_MorePyMC_PyMC3.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-little-more-on-pymc3">
   A little more on PyMC3
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-context">
     Model Context
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pymc3-variables">
     PyMC3 Variables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initializing-stochastic-variables">
       Initializing Stochastic variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#deterministic-variables">
       Deterministic variables
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theano">
     Theano
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#including-observations-in-the-model">
     Including observations in the Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling-approaches">
   Modeling approaches
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#same-story-different-ending">
     Same story; different ending.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-bayesian-a-b-testing">
       Example: Bayesian A/B testing
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-case">
     A Simple Case
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-and-b-together">
     <em>
      A
     </em>
     and
     <em>
      B
     </em>
     Together
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-algorithm-for-human-deceit">
   An algorithm for human deceit
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-binomial-distribution">
     The Binomial Distribution
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-cheating-among-students">
       Example: Cheating among students
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alternative-pymc3-model">
     Alternative PyMC3 Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-pymc3-tricks">
     More PyMC3 Tricks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#protip-arrays-of-pymc3-variables">
       Protip: Arrays of PyMC3 variables
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#example-challenger-space-shuttle-disaster-span-id-challenger">
         Example: Challenger Space Shuttle Disaster
         <span id="challenger">
         </span>
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distributions">
     Normal distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-about-the-day-of-the-challenger-disaster">
     What about the day of the Challenger disaster?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#is-our-model-appropriate">
     Is our model appropriate?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercises">
       Exercises
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#references">
     References
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="chapter-2">
<h1>Chapter 2<a class="headerlink" href="#chapter-2" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Original</span> <span class="pre">content</span> <span class="pre">created</span> <span class="pre">by</span> <span class="pre">Cam</span> <span class="pre">Davidson-Pilon</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">Ported</span> <span class="pre">to</span> <span class="pre">Python</span> <span class="pre">3</span> <span class="pre">and</span> <span class="pre">PyMC3</span> <span class="pre">by</span> <span class="pre">Max</span> <span class="pre">Margenot</span> <span class="pre">(&#64;clean_utensils)</span> <span class="pre">and</span> <span class="pre">Thomas</span> <span class="pre">Wiecki</span> <span class="pre">(&#64;twiecki)</span> <span class="pre">at</span> <span class="pre">Quantopian</span> <span class="pre">(&#64;quantopian)</span></code></p>
<hr class="docutils" />
<p>This chapter introduces more PyMC3 syntax and variables and ways to think about how to model a system from a Bayesian perspective. It also contains tips and data visualization techniques for assessing goodness-of-fit for your Bayesian model.</p>
<div class="section" id="a-little-more-on-pymc3">
<h2>A little more on PyMC3<a class="headerlink" href="#a-little-more-on-pymc3" title="Permalink to this headline">¶</a></h2>
<div class="section" id="model-context">
<h3>Model Context<a class="headerlink" href="#model-context" title="Permalink to this headline">¶</a></h3>
<p>In PyMC3, we typically handle all the variables we want in our model within the context of the <code class="docutils literal notranslate"><span class="pre">Model</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">parameter</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;poisson_param&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">data_generator</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s2">&quot;data_generator&quot;</span><span class="p">,</span> <span class="n">parameter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied log-transform to poisson_param and added transformed poisson_param_log_ to model.
</pre></div>
</div>
</div>
</div>
<p>This is an extra layer of convenience compared to PyMC. Any variables created within a given <code class="docutils literal notranslate"><span class="pre">Model</span></code>’s context will be automatically assigned to that model. If you try to define a variable outside of the context of a model, you will get an error.</p>
<p>We can continue to work within the context of the same model by using <code class="docutils literal notranslate"><span class="pre">with</span></code> with the name of the model object that we have already created.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">data_plus_one</span> <span class="o">=</span> <span class="n">data_generator</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>We can examine the same variables outside of the model context once they have been defined, but to define more variables that the model will recognize they have to be within the context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameter</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(0.693147177890573)
</pre></div>
</div>
</div>
</div>
<p>Each variable assigned to a model will be defined with its own name, the first string parameter (we will cover this further in the variables section). To create a different model object with the same name as one we have used previously, we need only run the first block of code again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="n">data_generator</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s2">&quot;data_generator&quot;</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied log-transform to theta and added transformed theta_log_ to model.
</pre></div>
</div>
</div>
</div>
<p>We can also define an entirely separate model. Note that we are free to name our models whatever we like, so if we do not want to overwrite an old model we need only make another.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">ab_testing</span><span class="p">:</span>
    <span class="n">p_A</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;P(A)&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p_B</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;P(B)&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied interval-transform to P(A) and added transformed P(A)_interval_ to model.
Applied interval-transform to P(B) and added transformed P(B)_interval_ to model.
</pre></div>
</div>
</div>
</div>
<p>You probably noticed that PyMC3 will often give you notifications about transformations when you add variables to your model. These transformations are done internally by PyMC3 to modify the space that the variable is sampled in (when we get to actually sampling the model). This is an internal feature which helps with the convergence of our samples to the posterior distribution and serves to improve the results.</p>
</div>
<div class="section" id="pymc3-variables">
<h3>PyMC3 Variables<a class="headerlink" href="#pymc3-variables" title="Permalink to this headline">¶</a></h3>
<p>All PyMC3 variables have an initial value (i.e. test value). Using the same variables from before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parameter.tag.test_value =&quot;</span><span class="p">,</span> <span class="n">parameter</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;data_generator.tag.test_value =&quot;</span><span class="p">,</span> <span class="n">data_generator</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;data_plus_one.tag.test_value =&quot;</span><span class="p">,</span> <span class="n">data_plus_one</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>parameter.tag.test_value = 0.693147177890573
data_generator.tag.test_value = 0
data_plus_one.tag.test_value = 1
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">test_value</span></code> is used only for the model, as the starting point for sampling if no other start is specified. It will not change as a result of sampling. This initial state can be changed at variable creation by specifying a value for the <code class="docutils literal notranslate"><span class="pre">testval</span></code> parameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">parameter</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;poisson_param&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">parameter.tag.test_value =&quot;</span><span class="p">,</span> <span class="n">parameter</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied log-transform to poisson_param and added transformed poisson_param_log_ to model.

parameter.tag.test_value = 0.49999999904767284
</pre></div>
</div>
</div>
</div>
<p>This can be helpful if you are using a more unstable prior that may require a better starting point.</p>
<p>PyMC3 is concerned with two types of programming variables: stochastic and deterministic.</p>
<ul class="simple">
<li><p><em>stochastic variables</em> are variables that are not deterministic, i.e., even if you knew all the values of the variables’ parameters and components, it would still be random. Included in this category are instances of classes <code class="docutils literal notranslate"><span class="pre">Poisson</span></code>, <code class="docutils literal notranslate"><span class="pre">DiscreteUniform</span></code>, and <code class="docutils literal notranslate"><span class="pre">Exponential</span></code>.</p></li>
<li><p><em>deterministic variables</em> are variables that are not random if the variables’ parameters and components were known. This might be confusing at first: a quick mental check is <em>if I knew all of variable <code class="docutils literal notranslate"><span class="pre">foo</span></code>’s component variables, I could determine what <code class="docutils literal notranslate"><span class="pre">foo</span></code>’s value is.</em></p></li>
</ul>
<p>We will detail each below.</p>
<div class="section" id="initializing-stochastic-variables">
<h4>Initializing Stochastic variables<a class="headerlink" href="#initializing-stochastic-variables" title="Permalink to this headline">¶</a></h4>
<p>Initializing a stochastic, or random, variable requires a <code class="docutils literal notranslate"><span class="pre">name</span></code> argument, plus additional parameters that are class specific. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">some_variable</span> <span class="pre">=</span> <span class="pre">pm.DiscreteUniform(&quot;discrete_uni_var&quot;,</span> <span class="pre">0,</span> <span class="pre">4)</span></code></p>
<p>where 0, 4 are the <code class="docutils literal notranslate"><span class="pre">DiscreteUniform</span></code>-specific lower and upper bound on the random variable. The <a class="reference external" href="http://pymc-devs.github.io/pymc3/api.html">PyMC3 docs</a> contain the specific parameters for stochastic variables. (Or use <code class="docutils literal notranslate"><span class="pre">??</span></code> if you are using IPython!)</p>
<p>The <code class="docutils literal notranslate"><span class="pre">name</span></code> attribute is used to retrieve the posterior distribution later in the analysis, so it is best to use a descriptive name. Typically, I use the Python variable’s name as the <code class="docutils literal notranslate"><span class="pre">name</span></code>.</p>
<p>For multivariable problems, rather than creating a Python array of stochastic variables, addressing the <code class="docutils literal notranslate"><span class="pre">shape</span></code> keyword in the call to a stochastic variable creates multivariate array of (independent) stochastic variables. The array behaves like a NumPy array when used like one, and references to its <code class="docutils literal notranslate"><span class="pre">tag.test_value</span></code> attribute return NumPy arrays.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">shape</span></code> argument also solves the annoying case where you may have many variables <span class="math notranslate nohighlight">\(\beta_i, \; i = 1,...,N\)</span> you wish to model. Instead of creating arbitrary names and variables for each one, like:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>beta_1 = pm.Uniform(&quot;beta_1&quot;, 0, 1)
beta_2 = pm.Uniform(&quot;beta_2&quot;, 0, 1)
...
</pre></div>
</div>
<p>we can instead wrap them into a single variable:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>betas = pm.Uniform(&quot;betas&quot;, 0, 1, shape=N)
</pre></div>
</div>
</div>
<div class="section" id="deterministic-variables">
<h4>Deterministic variables<a class="headerlink" href="#deterministic-variables" title="Permalink to this headline">¶</a></h4>
<p>We can create a deterministic variable similarly to how we create a stochastic variable. We simply call up the <code class="docutils literal notranslate"><span class="pre">Deterministic</span></code> class in PyMC3 and pass in the function that we desire</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>deterministic_variable = pm.Deterministic(&quot;deterministic variable&quot;, some_function_of_variables)
</pre></div>
</div>
<p>For all purposes, we can treat the object <code class="docutils literal notranslate"><span class="pre">some_deterministic_var</span></code> as a variable and not a Python function.</p>
<p>Calling <code class="docutils literal notranslate"><span class="pre">pymc3.Deterministic</span></code> is the most obvious way, but not the only way, to create deterministic variables. Elementary operations, like addition, exponentials etc. implicitly create deterministic variables. For example, the following returns a deterministic variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">lambda_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;lambda_1&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">lambda_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;lambda_2&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">DiscreteUniform</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">new_deterministic_variable</span> <span class="o">=</span> <span class="n">lambda_1</span> <span class="o">+</span> <span class="n">lambda_2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied log-transform to lambda_1 and added transformed lambda_1_log_ to model.
Applied log-transform to lambda_2 and added transformed lambda_2_log_ to model.
</pre></div>
</div>
</div>
</div>
<p>If we want a <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> variable to actually be tracked by our sampling, however, we need to define it explicitly as a named <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> variable with the constructor.</p>
<p>The use of the <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> variable was seen in the previous chapter’s text-message example.  Recall the model for <span class="math notranslate nohighlight">\(\lambda\)</span> looked like:</p>
<div class="math notranslate nohighlight">
\[
\lambda = 
\begin{cases}\lambda_1  &amp; \text{if } t \lt \tau \cr
\lambda_2 &amp; \text{if } t \ge \tau
\end{cases}
\]</div>
<p>And in PyMC3 code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">n_data_points</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># in CH1 we had ~70 data points</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_data_points</span><span class="p">)</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">tau</span> <span class="o">&gt;=</span> <span class="n">idx</span><span class="p">,</span> <span class="n">lambda_1</span><span class="p">,</span> <span class="n">lambda_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Clearly, if <span class="math notranslate nohighlight">\(\tau, \lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span> are known, then <span class="math notranslate nohighlight">\(\lambda\)</span> is known completely, hence it is a deterministic variable. We use the <code class="docutils literal notranslate"><span class="pre">switch</span></code> function here to change from <span class="math notranslate nohighlight">\(\lambda_1\)</span> to <span class="math notranslate nohighlight">\(\lambda_2\)</span> at the appropriate time. This function is directly from the <code class="docutils literal notranslate"><span class="pre">theano</span></code> package, which we will discuss in the next section.</p>
<p>Inside a <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> variable, the stochastic variables passed in behave like scalars or NumPy arrays (if multivariable). We can do whatever we want with them as long as the dimensions match up in our calculations.</p>
<p>For example, running the following:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def subtract(x, y):
    return x - y

stochastic_1 = pm.Uniform(&quot;U_1&quot;, 0, 1)
stochastic_2 = pm.Uniform(&quot;U_2&quot;, 0, 1)

det_1 = pm.Deterministic(&quot;Delta&quot;, subtract(stochastic_1, stochastic_2))
</pre></div>
</div>
<p>Is perfectly valid PyMC3 code. Saying that our expressions behave like NumPy arrays is not exactly honest here, however. The main catch is that the expression that we are making <em>must</em> be compatible with <code class="docutils literal notranslate"><span class="pre">theano</span></code> tensors, which we will cover in the next section. Feel free to define whatever functions that you need in order to compose your model. However, if you need to do any array-like calculations that would require NumPy functions, make sure you use their equivalents in <code class="docutils literal notranslate"><span class="pre">theano</span></code>.</p>
</div>
</div>
<div class="section" id="theano">
<h3>Theano<a class="headerlink" href="#theano" title="Permalink to this headline">¶</a></h3>
<p>The majority of the heavy lifting done by PyMC3 is taken care of with the <code class="docutils literal notranslate"><span class="pre">theano</span></code> package. The notation in <code class="docutils literal notranslate"><span class="pre">theano</span></code> is remarkably similar to NumPy. It also supports many of the familiar computational elements of NumPy. However, while NumPy directly executes computations, e.g. when you run <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code>, <code class="docutils literal notranslate"><span class="pre">theano</span></code> instead builds up a “compute graph” that tracks that you want to perform the <code class="docutils literal notranslate"><span class="pre">+</span></code> operation on the elements <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>. Only when you <code class="docutils literal notranslate"><span class="pre">eval()</span></code> a <code class="docutils literal notranslate"><span class="pre">theano</span></code> expression does the computation take place (i.e. <code class="docutils literal notranslate"><span class="pre">theano</span></code> is lazy evaluated). Once the compute graph is built, we can perform all kinds of mathematical optimizations (e.g. simplifications), compute gradients via autodiff, compile the entire graph to C to run at machine speed, and also compile it to run on the GPU. PyMC3 is basically a collection of <code class="docutils literal notranslate"><span class="pre">theano</span></code> symbolic expressions for various probability distributions that are combined to one big compute graph making up the whole model log probability, and a collection of inference algorithms that use that graph to compute probabilities and gradients. For practical purposes, what this means is that in order to build certain models we sometimes have to use <code class="docutils literal notranslate"><span class="pre">theano</span></code>.</p>
<p>Let’s write some PyMC3 code that involves <code class="docutils literal notranslate"><span class="pre">theano</span></code> calculations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">tt</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">theano_test</span><span class="p">:</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">])</span>
    
    <span class="n">assignment</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s2">&quot;assignment&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied interval-transform to p and added transformed p_interval_ to model.
</pre></div>
</div>
</div>
</div>
<p>Here we use <code class="docutils literal notranslate"><span class="pre">theano</span></code>’s <code class="docutils literal notranslate"><span class="pre">stack()</span></code> function in the same way we would use one of NumPy’s stacking functions: to combine our two separate variables, <code class="docutils literal notranslate"><span class="pre">p1</span></code> and <code class="docutils literal notranslate"><span class="pre">p2</span></code>, into a vector with <span class="math notranslate nohighlight">\(2\)</span> elements. The stochastic <code class="docutils literal notranslate"><span class="pre">categorical</span></code> variable does not understand what we mean if we pass a NumPy array of <code class="docutils literal notranslate"><span class="pre">p1</span></code> and <code class="docutils literal notranslate"><span class="pre">p2</span></code> to it because they are both <code class="docutils literal notranslate"><span class="pre">theano</span></code> variables. Stacking them like this combines them into one <code class="docutils literal notranslate"><span class="pre">theano</span></code> variable that we can use as the complementary pair of probabilities for our two categories.</p>
<p>Throughout the course of this book we use several <code class="docutils literal notranslate"><span class="pre">theano</span></code> functions to help construct our models. If you have more interest in looking at <code class="docutils literal notranslate"><span class="pre">theano</span></code> itself, be sure to check out the <a class="reference external" href="http://deeplearning.net/software/theano/library/">documentation</a>.</p>
<p>After these technical considerations, we can get back to defining our model!</p>
</div>
<div class="section" id="including-observations-in-the-model">
<h3>Including observations in the Model<a class="headerlink" href="#including-observations-in-the-model" title="Permalink to this headline">¶</a></h3>
<p>At this point, it may not look like it, but we have fully specified our priors. For example, we can ask and answer questions like “What does my prior distribution of <span class="math notranslate nohighlight">\(\lambda_1\)</span> look like?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">IPython.core.pylabtools</span> <span class="kn">import</span> <span class="n">figsize</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>


<span class="n">samples</span> <span class="o">=</span> <span class="n">lambda_1</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;stepfilled&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prior distribution for $\lambda_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_26_0.png" src="../_images/Ch2_MorePyMC_PyMC3_26_0.png" />
</div>
</div>
<p>To frame this in the notation of the first chapter, though this is a slight abuse of notation, we have specified <span class="math notranslate nohighlight">\(P(A)\)</span>. Our next goal is to include data/evidence/observations <span class="math notranslate nohighlight">\(X\)</span> into our model.</p>
<p>PyMC3 stochastic variables have a keyword argument <code class="docutils literal notranslate"><span class="pre">observed</span></code>. The keyword <code class="docutils literal notranslate"><span class="pre">observed</span></code> has a very simple role: fix the variable’s current value to be the given data, typically a NumPy <code class="docutils literal notranslate"><span class="pre">array</span></code> or pandas <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">fixed_variable</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s2">&quot;fxd&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;value: &quot;</span><span class="p">,</span> <span class="n">fixed_variable</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>value:  [10  5]
</pre></div>
</div>
</div>
</div>
<p>This is how we include data into our models: initializing a stochastic variable to have a <em>fixed value</em>.</p>
<p>To complete our text message example, we fix the PyMC3 variable <code class="docutils literal notranslate"><span class="pre">observations</span></code> to the observed dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We&#39;re using some fake data here</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">35</span><span class="p">])</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[10 25 15 20 35]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="modeling-approaches">
<h2>Modeling approaches<a class="headerlink" href="#modeling-approaches" title="Permalink to this headline">¶</a></h2>
<p>A good starting thought to Bayesian modeling is to think about <em>how your data might have been generated</em>. Position yourself in an omniscient position, and try to imagine how <em>you</em> would recreate the dataset.</p>
<p>In the last chapter we investigated text message data. We begin by asking how our observations may have been generated:</p>
<ol>
<li><p>We started by thinking “what is the best random variable to describe this count data?” A Poisson random variable is a good candidate because it can represent count data. So we model the number of sms’s received as sampled from a Poisson distribution.</p></li>
<li><p>Next, we think, “Ok, assuming sms’s are Poisson-distributed, what do I need for the Poisson distribution?” Well, the Poisson distribution has a parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p></li>
<li><p>Do we know <span class="math notranslate nohighlight">\(\lambda\)</span>? No. In fact, we have a suspicion that there are <em>two</em> <span class="math notranslate nohighlight">\(\lambda\)</span> values, one for the earlier behaviour and one for the later behaviour. We don’t know when the behaviour switches though, but call the switchpoint <span class="math notranslate nohighlight">\(\tau\)</span>.</p></li>
<li><p>What is a good distribution for the two <span class="math notranslate nohighlight">\(\lambda\)</span>s? The exponential is good, as it assigns probabilities to positive real numbers. Well the exponential distribution has a parameter too, call it <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
<li><p>Do we know what the parameter <span class="math notranslate nohighlight">\(\alpha\)</span> might be? No. At this point, we could continue and assign a distribution to <span class="math notranslate nohighlight">\(\alpha\)</span>, but it’s better to stop once we reach a set level of ignorance: whereas we have a prior belief about <span class="math notranslate nohighlight">\(\lambda\)</span>, (“it probably changes over time”, “it’s likely between 10 and 30”, etc.), we don’t really have any strong beliefs about <span class="math notranslate nohighlight">\(\alpha\)</span>. So it’s best to stop here.</p>
<p>What is a good value for <span class="math notranslate nohighlight">\(\alpha\)</span> then? We think that the <span class="math notranslate nohighlight">\(\lambda\)</span>s are between 10-30, so if we set <span class="math notranslate nohighlight">\(\alpha\)</span> really low (which corresponds to larger probability on high values) we are not reflecting our prior well. Similar, a too-high alpha misses our prior belief as well. A good idea for <span class="math notranslate nohighlight">\(\alpha\)</span> as to reflect our belief is to set the value so that the mean of <span class="math notranslate nohighlight">\(\lambda\)</span>, given <span class="math notranslate nohighlight">\(\alpha\)</span>, is equal to our observed mean. This was shown in the last chapter.</p>
</li>
<li><p>We have no expert opinion of when <span class="math notranslate nohighlight">\(\tau\)</span> might have occurred. So we will suppose <span class="math notranslate nohighlight">\(\tau\)</span> is from a discrete uniform distribution over the entire timespan.</p></li>
</ol>
<p>Below we give a graphical visualization of this, where arrows denote <code class="docutils literal notranslate"><span class="pre">parent-child</span></code> relationships. (provided by the <a class="reference external" href="http://daft-pgm.org/">Daft Python library</a> )</p>
<img src="http://i.imgur.com/7J30oCG.png" width = 700/>
<p>PyMC3, and other probabilistic programming languages, have been designed to tell these data-generation <em>stories</em>. More generally, B. Cronin writes [5]:</p>
<blockquote>
<div><p>Probabilistic programming will unlock narrative explanations of data, one of the holy grails of business analytics and the unsung hero of scientific persuasion. People think in terms of stories - thus the unreasonable power of the anecdote to drive decision-making, well-founded or not. But existing analytics largely fails to provide this kind of story; instead, numbers seemingly appear out of thin air, with little of the causal context that humans prefer when weighing their options.</p>
</div></blockquote>
<div class="section" id="same-story-different-ending">
<h3>Same story; different ending.<a class="headerlink" href="#same-story-different-ending" title="Permalink to this headline">¶</a></h3>
<p>Interestingly, we can create <em>new datasets</em> by retelling the story.
For example, if we reverse the above steps, we can simulate a possible realization of the dataset.</p>
<p>1. Specify when the user’s behaviour switches by sampling from <span class="math notranslate nohighlight">\(\text{DiscreteUniform}(0, 80)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>59
</pre></div>
</div>
</div>
</div>
<p>2. Draw <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span> from an <span class="math notranslate nohighlight">\(\text{Exp}(\alpha)\)</span> distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mf">20.</span>
<span class="n">lambda_1</span><span class="p">,</span> <span class="n">lambda_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">lambda_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>49.7521280843 10.1226712418
</pre></div>
</div>
</div>
</div>
<p>3.  For days before <span class="math notranslate nohighlight">\(\tau\)</span>, represent the user’s received SMS count by sampling from <span class="math notranslate nohighlight">\(\text{Poi}(\lambda_1)\)</span>, and sample from  <span class="math notranslate nohighlight">\(\text{Poi}(\lambda_2)\)</span> for days after <span class="math notranslate nohighlight">\(\tau\)</span>. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">lambda_2</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">80</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>4. Plot the artificial dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">80</span><span class="p">),</span> <span class="n">data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">tau</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;user behaviour changed&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (days)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;count of text-msgs received&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Artificial dataset&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_39_0.png" src="../_images/Ch2_MorePyMC_PyMC3_39_0.png" />
</div>
</div>
<p>It is okay that our fictional dataset does not look like our observed dataset: the probability is incredibly small it indeed would. PyMC3’s engine is designed to find good parameters, <span class="math notranslate nohighlight">\(\lambda_i, \tau\)</span>, that maximize this probability.</p>
<p>The ability to generate artificial dataset is an interesting side effect of our modeling, and we will see that this ability is a very important method of Bayesian inference. We produce a few more datasets below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_artificial_sms_dataset</span><span class="p">():</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mf">20.</span>
    <span class="n">lambda_1</span><span class="p">,</span> <span class="n">lambda_2</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">lambda_2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">80</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">80</span><span class="p">),</span> <span class="n">data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">tau</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;user behaviour changed&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">);</span>

<span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;More example of artificial datasets&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plot_artificial_sms_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_41_0.png" src="../_images/Ch2_MorePyMC_PyMC3_41_0.png" />
</div>
</div>
<p>Later we will see how we use this to make predictions and test the appropriateness of our models.</p>
<div class="section" id="example-bayesian-a-b-testing">
<h4>Example: Bayesian A/B testing<a class="headerlink" href="#example-bayesian-a-b-testing" title="Permalink to this headline">¶</a></h4>
<p>A/B testing is a statistical design pattern for determining the difference of effectiveness between two different treatments. For example, a pharmaceutical company is interested in the effectiveness of drug A vs drug B. The company will test drug A on some fraction of their trials, and drug B on the other fraction (this fraction is often 1/2, but we will relax this assumption). After performing enough trials, the in-house statisticians sift through the data to determine which drug yielded better results.</p>
<p>Similarly, front-end web developers are interested in which design of their website yields more sales or some other metric of interest. They will route some fraction of visitors to site A, and the other fraction to site B, and record if the visit yielded a sale or not. The data is recorded (in real-time), and analyzed afterwards.</p>
<p>Often, the post-experiment analysis is done using something called a hypothesis test like <em>difference of means test</em> or <em>difference of proportions test</em>. This involves often misunderstood quantities like a “Z-score” and even more confusing “p-values” (please don’t ask). If you have taken a statistics course, you have probably been taught this technique (though not necessarily <em>learned</em> this technique). And if you were like me, you may have felt uncomfortable with their derivation – good: the Bayesian approach to this problem is much more natural.</p>
</div>
</div>
<div class="section" id="a-simple-case">
<h3>A Simple Case<a class="headerlink" href="#a-simple-case" title="Permalink to this headline">¶</a></h3>
<p>As this is a hacker book, we’ll continue with the web-dev example. For the moment, we will focus on the analysis of site A only. Assume that there is some true <span class="math notranslate nohighlight">\(0 \lt p_A \lt 1\)</span> probability that users who, upon shown site A, eventually purchase from the site. This is the true effectiveness of site A. Currently, this quantity is unknown to us.</p>
<p>Suppose site A was shown to <span class="math notranslate nohighlight">\(N\)</span> people, and <span class="math notranslate nohighlight">\(n\)</span> people purchased from the site. One might conclude hastily that <span class="math notranslate nohighlight">\(p_A = \frac{n}{N}\)</span>. Unfortunately, the <em>observed frequency</em> <span class="math notranslate nohighlight">\(\frac{n}{N}\)</span> does not necessarily equal <span class="math notranslate nohighlight">\(p_A\)</span> – there is a difference between the <em>observed frequency</em> and the <em>true frequency</em> of an event. The true frequency can be interpreted as the probability of an event occurring. For example, the true frequency of rolling a 1 on a 6-sided die is <span class="math notranslate nohighlight">\(\frac{1}{6}\)</span>. Knowing the true frequency of events like:</p>
<ul class="simple">
<li><p>fraction of users who make purchases,</p></li>
<li><p>frequency of social attributes,</p></li>
<li><p>percent of internet users with cats etc.</p></li>
</ul>
<p>are common requests we ask of Nature. Unfortunately, often Nature hides the true frequency from us and we must <em>infer</em> it from observed data.</p>
<p>The <em>observed frequency</em> is then the frequency we observe: say rolling the die 100 times you may observe 20 rolls of 1. The observed frequency, 0.2, differs from the true frequency, <span class="math notranslate nohighlight">\(\frac{1}{6}\)</span>. We can use Bayesian statistics to infer probable values of the true frequency using an appropriate prior and observed data.</p>
<p>With respect to our A/B example, we are interested in using what we know, <span class="math notranslate nohighlight">\(N\)</span> (the total trials administered) and <span class="math notranslate nohighlight">\(n\)</span> (the number of conversions), to estimate what <span class="math notranslate nohighlight">\(p_A\)</span>, the true frequency of buyers, might be.</p>
<p>To setup a Bayesian model, we need to assign prior distributions to our unknown quantities. <em>A priori</em>, what do we think <span class="math notranslate nohighlight">\(p_A\)</span> might be? For this example, we have no strong conviction about <span class="math notranslate nohighlight">\(p_A\)</span>, so for now, let’s assume <span class="math notranslate nohighlight">\(p_A\)</span> is uniform over [0,1]:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>

<span class="c1"># The parameters are the bounds of the Uniform.</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied interval-transform to p and added transformed p_interval_ to model.
</pre></div>
</div>
</div>
</div>
<p>Had we had stronger beliefs, we could have expressed them in the prior above.</p>
<p>For this example, consider <span class="math notranslate nohighlight">\(p_A = 0.05\)</span>, and <span class="math notranslate nohighlight">\(N = 1500\)</span> users shown site A, and we will simulate whether the user made a purchase or not. To simulate this from <span class="math notranslate nohighlight">\(N\)</span> trials, we will use a <em>Bernoulli</em> distribution: if  <span class="math notranslate nohighlight">\(X\ \sim \text{Ber}(p)\)</span>, then <span class="math notranslate nohighlight">\(X\)</span> is 1 with probability <span class="math notranslate nohighlight">\(p\)</span> and 0 with probability <span class="math notranslate nohighlight">\(1 - p\)</span>. Of course, in practice we do not know <span class="math notranslate nohighlight">\(p_A\)</span>, but we will use it here to simulate the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#set constants</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># remember, this is unknown.</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1500</span>

<span class="c1"># sample N Bernoulli random variables from Ber(0.05).</span>
<span class="c1"># each random variable has a 0.05 chance of being a 1.</span>
<span class="c1"># this is the data-generation step</span>
<span class="n">occurrences</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">p_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">occurrences</span><span class="p">)</span> <span class="c1"># Remember: Python treats True == 1, and False == 0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">occurrences</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1 0 1 ..., 0 0 0]
77
</pre></div>
</div>
</div>
</div>
<p>The observed frequency is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Occurrences.mean is equal to n/N.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;What is the observed frequency in Group A? </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">occurrences</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Does this equal the true frequency? </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">occurrences</span><span class="p">)</span> <span class="o">==</span> <span class="n">p_true</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>What is the observed frequency in Group A? 0.0513
Does this equal the true frequency? False
</pre></div>
</div>
</div>
</div>
<p>We combine the observations into the PyMC3 <code class="docutils literal notranslate"><span class="pre">observed</span></code> variable, and run our inference algorithm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#include the observations, which are Bernoulli</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">occurrences</span><span class="p">)</span>
    <span class="c1"># To be explained in chapter 3</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">()</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">18000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
    <span class="n">burned_trace</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="mi">1000</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> [-------100%-------] 18000 of 18000 in 1.7 sec. | SPS: 10329.7 | ETA: 0.0
</pre></div>
</div>
</div>
</div>
<p>We plot the posterior distribution of the unknown <span class="math notranslate nohighlight">\(p_A\)</span> below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior distribution of $p_A$, the true effectiveness of site A&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">p_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true $p_A$ (unknown)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;p&quot;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;stepfilled&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_52_0.png" src="../_images/Ch2_MorePyMC_PyMC3_52_0.png" />
</div>
</div>
<p>Our posterior distribution puts most weight near the true value of <span class="math notranslate nohighlight">\(p_A\)</span>, but also some weights in the tails. This is a measure of how uncertain we should be, given our observations. Try changing the number of observations, <code class="docutils literal notranslate"><span class="pre">N</span></code>, and observe how the posterior distribution changes.</p>
</div>
<div class="section" id="a-and-b-together">
<h3><em>A</em> and <em>B</em> Together<a class="headerlink" href="#a-and-b-together" title="Permalink to this headline">¶</a></h3>
<p>A similar analysis can be done for site B’s response data to determine the analogous <span class="math notranslate nohighlight">\(p_B\)</span>. But what we are really interested in is the <em>difference</em> between <span class="math notranslate nohighlight">\(p_A\)</span> and <span class="math notranslate nohighlight">\(p_B\)</span>. Let’s infer <span class="math notranslate nohighlight">\(p_A\)</span>, <span class="math notranslate nohighlight">\(p_B\)</span>, <em>and</em> <span class="math notranslate nohighlight">\(\text{delta} = p_A - p_B\)</span>, all at once. We can do this using PyMC3’s deterministic variables. (We’ll assume for this exercise that <span class="math notranslate nohighlight">\(p_B = 0.04\)</span>, so <span class="math notranslate nohighlight">\(\text{delta} = 0.01\)</span>, <span class="math notranslate nohighlight">\(N_B = 750\)</span> (significantly less than <span class="math notranslate nohighlight">\(N_A\)</span>) and we will simulate site B’s data like we did for site A’s data )</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="n">figsize</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1">#these two quantities are unknown to us.</span>
<span class="n">true_p_A</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">true_p_B</span> <span class="o">=</span> <span class="mf">0.04</span>

<span class="c1">#notice the unequal sample sizes -- no problem in Bayesian analysis.</span>
<span class="n">N_A</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">N_B</span> <span class="o">=</span> <span class="mi">750</span>

<span class="c1">#generate some observations</span>
<span class="n">observations_A</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">true_p_A</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N_A</span><span class="p">)</span>
<span class="n">observations_B</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">true_p_B</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N_B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Obs from Site A: &quot;</span><span class="p">,</span> <span class="n">observations_A</span><span class="p">[:</span><span class="mi">30</span><span class="p">],</span> <span class="s2">&quot;...&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Obs from Site B: &quot;</span><span class="p">,</span> <span class="n">observations_B</span><span class="p">[:</span><span class="mi">30</span><span class="p">],</span> <span class="s2">&quot;...&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Obs from Site A:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ...
Obs from Site B:  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] ...
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">observations_A</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">observations_B</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.042
0.0346666666667
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up the pymc3 model. Again assume Uniform priors for p_A and p_B.</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">p_A</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;p_A&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p_B</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;p_B&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Define the deterministic delta function. This is our unknown of interest.</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;delta&quot;</span><span class="p">,</span> <span class="n">p_A</span> <span class="o">-</span> <span class="n">p_B</span><span class="p">)</span>

    
    <span class="c1"># Set of observations, in this case we have two observation datasets.</span>
    <span class="n">obs_A</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;obs_A&quot;</span><span class="p">,</span> <span class="n">p_A</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">observations_A</span><span class="p">)</span>
    <span class="n">obs_B</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;obs_B&quot;</span><span class="p">,</span> <span class="n">p_B</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">observations_B</span><span class="p">)</span>

    <span class="c1"># To be explained in chapter 3.</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">()</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">20000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
    <span class="n">burned_trace</span><span class="o">=</span><span class="n">trace</span><span class="p">[</span><span class="mi">1000</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied interval-transform to p_A and added transformed p_A_interval_ to model.
Applied interval-transform to p_B and added transformed p_B_interval_ to model.
 [-------100%-------] 20000 of 20000 in 3.2 sec. | SPS: 6201.6 | ETA: 0.0
</pre></div>
</div>
</div>
</div>
<p>Below we plot the posterior distributions for the three unknowns:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_A_samples</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;p_A&quot;</span><span class="p">]</span>
<span class="n">p_B_samples</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;p_B&quot;</span><span class="p">]</span>
<span class="n">delta_samples</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;delta&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1">#histogram of posteriors</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">p_A_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior of $p_A$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#A60628&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">true_p_A</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true $p_A$ (unknown)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior distributions of $p_A$, $p_B$, and delta unknowns&quot;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">p_B_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior of $p_B$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#467821&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">true_p_B</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true $p_B$ (unknown)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">delta_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior of delta&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7A68A6&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">true_p_A</span> <span class="o">-</span> <span class="n">true_p_B</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true delta (unknown)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_59_0.png" src="../_images/Ch2_MorePyMC_PyMC3_59_0.png" />
</div>
</div>
<p>Notice that as a result of <code class="docutils literal notranslate"><span class="pre">N_B</span> <span class="pre">&lt;</span> <span class="pre">N_A</span></code>, i.e. we have less data from site B, our posterior distribution of <span class="math notranslate nohighlight">\(p_B\)</span> is fatter, implying we are less certain about the true value of <span class="math notranslate nohighlight">\(p_B\)</span> than we are of <span class="math notranslate nohighlight">\(p_A\)</span>.</p>
<p>With respect to the posterior distribution of <span class="math notranslate nohighlight">\(\text{delta}\)</span>, we can see that the majority of the distribution is above <span class="math notranslate nohighlight">\(\text{delta}=0\)</span>, implying there site A’s response is likely better than site B’s response. The probability this inference is incorrect is easily computable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Count the number of samples less than 0, i.e. the area under the curve</span>
<span class="c1"># before 0, represent the probability that site A is worse than site B.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probability site A is WORSE than site B: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> \
    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">delta_samples</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probability site A is BETTER than site B: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> \
    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">delta_samples</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Probability site A is WORSE than site B: 0.208
Probability site A is BETTER than site B: 0.792
</pre></div>
</div>
</div>
</div>
<p>If this probability is too high for comfortable decision-making, we can perform more trials on site B (as site B has less samples to begin with, each additional data point for site B contributes more inferential “power” than each additional data point for site A).</p>
<p>Try playing with the parameters <code class="docutils literal notranslate"><span class="pre">true_p_A</span></code>, <code class="docutils literal notranslate"><span class="pre">true_p_B</span></code>, <code class="docutils literal notranslate"><span class="pre">N_A</span></code>, and <code class="docutils literal notranslate"><span class="pre">N_B</span></code>, to see what the posterior of <span class="math notranslate nohighlight">\(\text{delta}\)</span> looks like. Notice in all this, the difference in sample sizes between site A and site B was never mentioned: it naturally fits into Bayesian analysis.</p>
<p>I hope the readers feel this style of A/B testing is more natural than hypothesis testing, which has probably confused more than helped practitioners. Later in this book, we will see two extensions of this model: the first to help dynamically adjust for bad sites, and the second will improve the speed of this computation by reducing the analysis to a single equation.</p>
</div>
</div>
<div class="section" id="an-algorithm-for-human-deceit">
<h2>An algorithm for human deceit<a class="headerlink" href="#an-algorithm-for-human-deceit" title="Permalink to this headline">¶</a></h2>
<p>Social data has an additional layer of interest as people are not always honest with responses, which adds a further complication into inference. For example, simply asking individuals “Have you ever cheated on a test?” will surely contain some rate of dishonesty. What you can say for certain is that the true rate is less than your observed rate (assuming individuals lie <em>only</em> about <em>not cheating</em>; I cannot imagine one who would admit “Yes” to cheating when in fact they hadn’t cheated).</p>
<p>To present an elegant solution to circumventing this dishonesty problem, and to demonstrate Bayesian modeling, we first need to introduce the binomial distribution.</p>
<div class="section" id="the-binomial-distribution">
<h3>The Binomial Distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this headline">¶</a></h3>
<p>The binomial distribution is one of the most popular distributions, mostly because of its simplicity and usefulness. Unlike the other distributions we have encountered thus far in the book, the binomial distribution has 2 parameters: <span class="math notranslate nohighlight">\(N\)</span>, a positive integer representing <span class="math notranslate nohighlight">\(N\)</span> trials or number of instances of potential events, and <span class="math notranslate nohighlight">\(p\)</span>, the probability of an event occurring in a single trial. Like the Poisson distribution, it is a discrete distribution, but unlike the Poisson distribution, it only weighs integers from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N\)</span>. The mass distribution looks like:</p>
<div class="math notranslate nohighlight">
\[P( X = k ) =  {{N}\choose{k}}  p^k(1-p)^{N-k}\]</div>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a binomial random variable with parameters <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(N\)</span>, denoted <span class="math notranslate nohighlight">\(X \sim \text{Bin}(N,p)\)</span>, then <span class="math notranslate nohighlight">\(X\)</span> is the number of events that occurred in the <span class="math notranslate nohighlight">\(N\)</span> trials (obviously <span class="math notranslate nohighlight">\(0 \le X \le N\)</span>). The larger <span class="math notranslate nohighlight">\(p\)</span> is (while still remaining between 0 and 1), the more events are likely to occur. The expected value of a binomial is equal to <span class="math notranslate nohighlight">\(Np\)</span>. Below we plot the mass probability distribution for varying parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="n">binomial</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">.4</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">.9</span><span class="p">)]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">,</span> <span class="s2">&quot;#A60628&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">_x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">binomial</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$N$: </span><span class="si">%d</span><span class="s2">, $p$: </span><span class="si">%.1f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span>
            <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$k$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$P(X = k)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Probability mass distributions of binomial random variables&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_64_0.png" src="../_images/Ch2_MorePyMC_PyMC3_64_0.png" />
</div>
</div>
<p>The special case when <span class="math notranslate nohighlight">\(N = 1\)</span> corresponds to the Bernoulli distribution. There is another connection between Bernoulli and Binomial random variables. If we have <span class="math notranslate nohighlight">\(X_1, X_2, ... , X_N\)</span> Bernoulli random variables with the same <span class="math notranslate nohighlight">\(p\)</span>, then <span class="math notranslate nohighlight">\(Z = X_1 + X_2 + ... + X_N \sim \text{Binomial}(N, p )\)</span>.</p>
<p>The expected value of a Bernoulli random variable is <span class="math notranslate nohighlight">\(p\)</span>. This can be seen by noting the more general Binomial random variable has expected value <span class="math notranslate nohighlight">\(Np\)</span> and setting <span class="math notranslate nohighlight">\(N=1\)</span>.</p>
<div class="section" id="example-cheating-among-students">
<h4>Example: Cheating among students<a class="headerlink" href="#example-cheating-among-students" title="Permalink to this headline">¶</a></h4>
<p>We will use the binomial distribution to determine the frequency of students cheating during an exam. If we let <span class="math notranslate nohighlight">\(N\)</span> be the total number of students who took the exam, and assuming each student is interviewed post-exam (answering without consequence), we will receive integer <span class="math notranslate nohighlight">\(X\)</span> “Yes I did cheat” answers. We then find the posterior distribution of <span class="math notranslate nohighlight">\(p\)</span>, given <span class="math notranslate nohighlight">\(N\)</span>, some specified prior on <span class="math notranslate nohighlight">\(p\)</span>, and observed data <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>This is a completely absurd model. No student, even with a free-pass against punishment, would admit to cheating. What we need is a better <em>algorithm</em> to ask students if they had cheated. Ideally the algorithm should encourage individuals to be honest while preserving privacy. The following proposed algorithm is a solution I greatly admire for its ingenuity and effectiveness:</p>
<blockquote>
<div><p>In the interview process for each student, the student flips a coin, hidden from the interviewer. The student agrees to answer honestly if the coin comes up heads. Otherwise, if the coin comes up tails, the student (secretly) flips the coin again, and answers “Yes, I did cheat” if the coin flip lands heads, and “No, I did not cheat”, if the coin flip lands tails. This way, the interviewer does not know if a “Yes” was the result of a guilty plea, or a Heads on a second coin toss. Thus privacy is preserved and the researchers receive honest answers.</p>
</div></blockquote>
<p>I call this the Privacy Algorithm. One could of course argue that the interviewers are still receiving false data since some <em>Yes</em>’s are not confessions but instead randomness, but an alternative perspective is that the researchers are discarding approximately half of their original dataset since half of the responses will be noise. But they have gained a systematic data generation process that can be modeled. Furthermore, they do not have to incorporate (perhaps somewhat naively) the possibility of deceitful answers. We can use PyMC3 to dig through this noisy model, and find a posterior distribution for the true frequency of liars.</p>
<p>Suppose 100 students are being surveyed for cheating, and we wish to find <span class="math notranslate nohighlight">\(p\)</span>, the proportion of cheaters. There are a few ways we can model this in PyMC3. I’ll demonstrate the most explicit way, and later show a simplified version. Both versions arrive at the same inference. In our data-generation model, we sample <span class="math notranslate nohighlight">\(p\)</span>, the true proportion of cheaters, from a prior. Since we are quite ignorant about <span class="math notranslate nohighlight">\(p\)</span>, we will assign it a <span class="math notranslate nohighlight">\(\text{Uniform}(0,1)\)</span> prior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;freq_cheating&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied interval-transform to freq_cheating and added transformed freq_cheating_interval_ to model.
</pre></div>
</div>
</div>
</div>
<p>Again, thinking of our data-generation model, we assign Bernoulli random variables to the 100 students: 1 implies they cheated and 0 implies they did not.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">true_answers</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;truths&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>If we carry out the algorithm, the next step that occurs is the first coin-flip each student makes. This can be modeled again by sampling 100 Bernoulli random variables with <span class="math notranslate nohighlight">\(p=1/2\)</span>: denote a 1 as a <em>Heads</em> and 0 a <em>Tails</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">first_coin_flips</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;first_flips&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_coin_flips</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1
 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1
 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0]
</pre></div>
</div>
</div>
</div>
<p>Although <em>not everyone</em> flips a second time, we can still model the possible realization of second coin-flips:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">second_coin_flips</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;second_flips&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Using these variables, we can return a possible realization of the <em>observed proportion</em> of “Yes” responses. We do this using a PyMC3 <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">tt</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">first_coin_flips</span><span class="o">*</span><span class="n">true_answers</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">first_coin_flips</span><span class="p">)</span><span class="o">*</span><span class="n">second_coin_flips</span>
    <span class="n">observed_proportion</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;observed_proportion&quot;</span><span class="p">,</span> <span class="n">tt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The line <code class="docutils literal notranslate"><span class="pre">fc*t_a</span> <span class="pre">+</span> <span class="pre">(1-fc)*sc</span></code> contains the heart of the Privacy algorithm. Elements in this array are 1 <em>if and only if</em> i) the first toss is heads and the student cheated or ii) the first toss is tails, and the second is heads, and are 0 else. Finally, the last line sums this vector and divides by <code class="docutils literal notranslate"><span class="pre">float(N)</span></code>, produces a proportion.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">observed_proportion</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(0.5600000023841858)
</pre></div>
</div>
</div>
</div>
<p>Next we need a dataset. After performing our coin-flipped interviews the researchers received 35 “Yes” responses. To put this into a relative perspective, if there truly were no cheaters, we should expect to see on average 1/4 of all responses being a “Yes” (half chance of having first coin land Tails, and another half chance of having second coin land Heads), so about 25 responses in a cheat-free world. On the other hand, if <em>all students cheated</em>, we should expected to see approximately 3/4 of all responses be “Yes”.</p>
<p>The researchers observe a Binomial random variable, with <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">=</span> <span class="pre">100</span></code> and <code class="docutils literal notranslate"><span class="pre">p</span> <span class="pre">=</span> <span class="pre">observed_proportion</span></code> with <code class="docutils literal notranslate"><span class="pre">value</span> <span class="pre">=</span> <span class="pre">35</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="mi">35</span>

<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">observed_proportion</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Below we add all the variables of interest to a <code class="docutils literal notranslate"><span class="pre">Model</span></code> container and run our black-box algorithm over the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># To be explained in Chapter 3!</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">(</span><span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
    <span class="n">burned_trace</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="mi">15000</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Assigned BinaryGibbsMetropolis to truths
Assigned BinaryGibbsMetropolis to first_flips
Assigned BinaryGibbsMetropolis to second_flips
 [-------100%-------] 40000 of 40000 in 1891.9 sec. | SPS: 21.1 | ETA: -0.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p_trace</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;freq_cheating&quot;</span><span class="p">][</span><span class="mi">15000</span><span class="p">:]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">p_trace</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;stepfilled&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior distribution&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="mf">.05</span><span class="p">,</span> <span class="mf">.35</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_83_0.png" src="../_images/Ch2_MorePyMC_PyMC3_83_0.png" />
</div>
</div>
<p>With regards to the above plot, we are still pretty uncertain about what the true frequency of cheaters might be, but we have narrowed it down to a range between 0.05 to 0.35 (marked by the solid lines). This is pretty good, as <em>a priori</em> we had no idea how many students might have cheated (hence the uniform distribution for our prior). On the other hand, it is also pretty bad since there is a .3 length window the true value most likely lives in. Have we even gained anything, or are we still too uncertain about the true frequency?</p>
<p>I would argue, yes, we have discovered something. It is implausible, according to our posterior, that there are <em>no cheaters</em>, i.e. the posterior assigns low probability to <span class="math notranslate nohighlight">\(p=0\)</span>. Since we started with an uniform prior, treating all values of <span class="math notranslate nohighlight">\(p\)</span> as equally plausible, but the data ruled out <span class="math notranslate nohighlight">\(p=0\)</span> as a possibility, we can be confident that there were cheaters.</p>
<p>This kind of algorithm can be used to gather private information from users and be <em>reasonably</em> confident that the data, though noisy, is truthful.</p>
</div>
</div>
<div class="section" id="alternative-pymc3-model">
<h3>Alternative PyMC3 Model<a class="headerlink" href="#alternative-pymc3-model" title="Permalink to this headline">¶</a></h3>
<p>Given a value for <span class="math notranslate nohighlight">\(p\)</span> (which from our god-like position we know), we can find the probability the student will answer yes:</p>
<p>\begin{align}
P(\text{“Yes”}) = &amp; P( \text{Heads on first coin} )P( \text{cheater} ) + P( \text{Tails on first coin} )P( \text{Heads on second coin} ) \
&amp; = \frac{1}{2}p + \frac{1}{2}\frac{1}{2}\
&amp; = \frac{p}{2} + \frac{1}{4}
\end{align}</p>
<p>Thus, knowing <span class="math notranslate nohighlight">\(p\)</span> we know the probability a student will respond “Yes”. In PyMC3, we can create a deterministic function to evaluate the probability of responding “Yes”, given <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;freq_cheating&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p_skewed</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;p_skewed&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">p</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied interval-transform to freq_cheating and added transformed freq_cheating_interval_ to model.
</pre></div>
</div>
</div>
</div>
<p>I could have typed <code class="docutils literal notranslate"><span class="pre">p_skewed</span>&#160; <span class="pre">=</span> <span class="pre">0.5*p</span> <span class="pre">+</span> <span class="pre">0.25</span></code> instead for a one-liner, as the elementary operations of addition and scalar multiplication will implicitly create a <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> variable, but I wanted to make the deterministic boilerplate explicit for clarity’s sake.</p>
<p>If we know the probability of respondents saying “Yes”, which is <code class="docutils literal notranslate"><span class="pre">p_skewed</span></code>, and we have <span class="math notranslate nohighlight">\(N=100\)</span> students, the number of “Yes” responses is a binomial random variable with parameters <code class="docutils literal notranslate"><span class="pre">N</span></code> and <code class="docutils literal notranslate"><span class="pre">p_skewed</span></code>.</p>
<p>This is where we include our observed 35 “Yes” responses. In the declaration of the <code class="docutils literal notranslate"><span class="pre">pm.Binomial</span></code>, we include <code class="docutils literal notranslate"><span class="pre">value</span> <span class="pre">=</span> <span class="pre">35</span></code> and <code class="docutils literal notranslate"><span class="pre">observed</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">yes_responses</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s2">&quot;number_cheaters&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">p_skewed</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="mi">35</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Below we add all the variables of interest to a <code class="docutils literal notranslate"><span class="pre">Model</span></code> container and run our black-box algorithm over the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># To Be Explained in Chapter 3!</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">()</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">25000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
    <span class="n">burned_trace</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="mi">2500</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> [-------100%-------] 25000 of 25000 in 2.1 sec. | SPS: 12171.2 | ETA: 0.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p_trace</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;freq_cheating&quot;</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">p_trace</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;stepfilled&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior distribution&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="mf">.05</span><span class="p">,</span> <span class="mf">.35</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_91_0.png" src="../_images/Ch2_MorePyMC_PyMC3_91_0.png" />
</div>
</div>
</div>
<div class="section" id="more-pymc3-tricks">
<h3>More PyMC3 Tricks<a class="headerlink" href="#more-pymc3-tricks" title="Permalink to this headline">¶</a></h3>
<div class="section" id="protip-arrays-of-pymc3-variables">
<h4>Protip: Arrays of PyMC3 variables<a class="headerlink" href="#protip-arrays-of-pymc3-variables" title="Permalink to this headline">¶</a></h4>
<p>There is no reason why we cannot store multiple heterogeneous PyMC3 variables in a Numpy array. Just remember to set the <code class="docutils literal notranslate"><span class="pre">dtype</span></code> of the array to <code class="docutils literal notranslate"><span class="pre">object</span></code> upon initialization. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;x_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Applied log-transform to x_0 and added transformed x_0_log_ to model.
Applied log-transform to x_1 and added transformed x_1_log_ to model.
Applied log-transform to x_2 and added transformed x_2_log_ to model.
Applied log-transform to x_3 and added transformed x_3_log_ to model.
Applied log-transform to x_4 and added transformed x_4_log_ to model.
Applied log-transform to x_5 and added transformed x_5_log_ to model.
Applied log-transform to x_6 and added transformed x_6_log_ to model.
Applied log-transform to x_7 and added transformed x_7_log_ to model.
Applied log-transform to x_8 and added transformed x_8_log_ to model.
Applied log-transform to x_9 and added transformed x_9_log_ to model.
</pre></div>
</div>
</div>
</div>
<p>The remainder of this chapter examines some practical examples of PyMC3 and PyMC3 modeling:</p>
<div class="section" id="example-challenger-space-shuttle-disaster-span-id-challenger">
<h5>Example: Challenger Space Shuttle Disaster <span id="challenger"/><a class="headerlink" href="#example-challenger-space-shuttle-disaster-span-id-challenger" title="Permalink to this headline">¶</a></h5>
<p>On January 28, 1986, the twenty-fifth flight of the U.S. space shuttle program ended in disaster when one of the rocket boosters of the Shuttle Challenger exploded shortly after lift-off, killing all seven crew members. The presidential commission on the accident concluded that it was caused by the failure of an O-ring in a field joint on the rocket booster, and that this failure was due to a faulty design that made the O-ring unacceptably sensitive to a number of factors including outside temperature. Of the previous 24 flights, data were available on failures of O-rings on 23, (one was lost at sea), and these data were discussed on the evening preceding the Challenger launch, but unfortunately only the data corresponding to the 7 flights on which there was a damage incident were considered important and these were thought to show no obvious trend. The data are shown below (see [1]):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">challenger_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;data/challenger_data.csv&quot;</span><span class="p">,</span> <span class="n">skip_header</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">missing_values</span><span class="o">=</span><span class="s2">&quot;NA&quot;</span><span class="p">,</span>
                                <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
<span class="c1">#drop the NA values</span>
<span class="n">challenger_data</span> <span class="o">=</span> <span class="n">challenger_data</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">challenger_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])]</span>

<span class="c1">#plot it, as a function of tempature (the first column)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Temp (F), O-Ring failure?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">challenger_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">challenger_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">challenger_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Damage Incident?&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Outside temperature (Fahrenheit)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Defects of the Space Shuttle O-Rings vs temperature&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temp (F), O-Ring failure?
[[ 66.   0.]
 [ 70.   1.]
 [ 69.   0.]
 [ 68.   0.]
 [ 67.   0.]
 [ 72.   0.]
 [ 73.   0.]
 [ 70.   0.]
 [ 57.   1.]
 [ 63.   1.]
 [ 70.   1.]
 [ 78.   0.]
 [ 67.   0.]
 [ 53.   1.]
 [ 67.   0.]
 [ 75.   0.]
 [ 70.   0.]
 [ 81.   0.]
 [ 76.   0.]
 [ 79.   0.]
 [ 75.   1.]
 [ 76.   0.]
 [ 58.   1.]]
</pre></div>
</div>
<img alt="../_images/Ch2_MorePyMC_PyMC3_96_1.png" src="../_images/Ch2_MorePyMC_PyMC3_96_1.png" />
</div>
</div>
<p>It looks clear that <em>the probability</em> of damage incidents occurring increases as the outside temperature decreases. We are interested in modeling the probability here because it does not look like there is a strict cutoff point between temperature and a damage incident occurring. The best we can do is ask “At temperature <span class="math notranslate nohighlight">\(t\)</span>, what is the probability of a damage incident?”. The goal of this example is to answer that question.</p>
<p>We need a function of temperature, call it <span class="math notranslate nohighlight">\(p(t)\)</span>, that is bounded between 0 and 1 (so as to model a probability) and changes from 1 to 0 as we increase temperature. There are actually many such functions, but the most popular choice is the <em>logistic function.</em></p>
<div class="math notranslate nohighlight">
\[p(t) = \frac{1}{ 1 + e^{ \;\beta t } } \]</div>
<p>In this model, <span class="math notranslate nohighlight">\(\beta\)</span> is the variable we are uncertain about. Below is the function plotted for <span class="math notranslate nohighlight">\(\beta = 1, 3, -5\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = 1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = 3$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = -5$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_98_0.png" src="../_images/Ch2_MorePyMC_PyMC3_98_0.png" />
</div>
</div>
<p>But something is missing. In the plot of the logistic function, the probability changes only near zero, but in our data above the probability changes around 65 to 70. We need to add a <em>bias</em> term to our logistic function:</p>
<div class="math notranslate nohighlight">
\[p(t) = \frac{1}{ 1 + e^{ \;\beta t + \alpha } } \]</div>
<p>Some plots are below, with differing <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = 1$&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = 3$&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = -5$&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = 1, \alpha = 1$&quot;</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = 3, \alpha = -2$&quot;</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#A60628&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta = -5, \alpha = 7$&quot;</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7A68A6&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_100_0.png" src="../_images/Ch2_MorePyMC_PyMC3_100_0.png" />
</div>
</div>
<p>Adding a constant term <span class="math notranslate nohighlight">\(\alpha\)</span> amounts to shifting the curve left or right (hence why it is called a <em>bias</em>).</p>
<p>Let’s start modeling this in PyMC3. The <span class="math notranslate nohighlight">\(\beta, \alpha\)</span> parameters have no reason to be positive, bounded or relatively large, so they are best modeled by a <em>Normal random variable</em>, introduced next.</p>
</div>
</div>
</div>
<div class="section" id="normal-distributions">
<h3>Normal distributions<a class="headerlink" href="#normal-distributions" title="Permalink to this headline">¶</a></h3>
<p>A Normal random variable, denoted <span class="math notranslate nohighlight">\(X \sim N(\mu, 1/\tau)\)</span>, has a distribution with two parameters: the mean, <span class="math notranslate nohighlight">\(\mu\)</span>, and the <em>precision</em>, <span class="math notranslate nohighlight">\(\tau\)</span>. Those familiar with the Normal distribution already have probably seen <span class="math notranslate nohighlight">\(\sigma^2\)</span> instead of <span class="math notranslate nohighlight">\(\tau^{-1}\)</span>. They are in fact reciprocals of each other. The change was motivated by simpler mathematical analysis and is an artifact of older Bayesian methods. Just remember: the smaller <span class="math notranslate nohighlight">\(\tau\)</span>, the larger the spread of the distribution (i.e. we are more uncertain); the larger <span class="math notranslate nohighlight">\(\tau\)</span>, the tighter the distribution (i.e. we are more certain). Regardless, <span class="math notranslate nohighlight">\(\tau\)</span> is always positive.</p>
<p>The probability density function of a <span class="math notranslate nohighlight">\(N( \mu, 1/\tau)\)</span> random variable is:</p>
<div class="math notranslate nohighlight">
\[ f(x | \mu, \tau) = \sqrt{\frac{\tau}{2\pi}} \exp\left( -\frac{\tau}{2} (x-\mu)^2 \right) \]</div>
<p>We plot some different density functions below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>

<span class="n">nor</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">tau</span> <span class="o">=</span> <span class="p">(</span><span class="mf">.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#348ABD&quot;</span><span class="p">,</span> <span class="s2">&quot;#A60628&quot;</span><span class="p">,</span> <span class="s2">&quot;#7A68A6&quot;</span><span class="p">]</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">colors</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_mu</span><span class="p">,</span> <span class="n">_tau</span><span class="p">,</span> <span class="n">_color</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nor</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">_tau</span><span class="p">),</span>
             <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\mu = </span><span class="si">%d</span><span class="s2">,\;</span><span class="se">\\</span><span class="s2">tau = </span><span class="si">%.1f</span><span class="s2">$&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">_mu</span><span class="p">,</span> <span class="n">_tau</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">_color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nor</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">_tau</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">_color</span><span class="p">,</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">.33</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;density function at $x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Probability distribution of three different Normal random </span><span class="se">\</span>
<span class="s2">variables&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_103_0.png" src="../_images/Ch2_MorePyMC_PyMC3_103_0.png" />
</div>
</div>
<p>A Normal random variable can be take on any real number, but the variable is very likely to be relatively close to <span class="math notranslate nohighlight">\(\mu\)</span>. In fact, the expected value of a Normal is equal to its <span class="math notranslate nohighlight">\(\mu\)</span> parameter:</p>
<div class="math notranslate nohighlight">
\[ E[ X | \mu, \tau] = \mu\]</div>
<p>and its variance is equal to the inverse of <span class="math notranslate nohighlight">\(\tau\)</span>:</p>
<div class="math notranslate nohighlight">
\[Var( X | \mu, \tau ) = \frac{1}{\tau}\]</div>
<p>Below we continue our modeling of the Challenger space craft:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>

<span class="n">temperature</span> <span class="o">=</span> <span class="n">challenger_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">challenger_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># defect or not?</span>

<span class="c1">#notice the`value` here. We explain why below.</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">tt</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">temperature</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>We have our probabilities, but how do we connect them to our observed data? A <em>Bernoulli</em> random variable with parameter <span class="math notranslate nohighlight">\(p\)</span>, denoted <span class="math notranslate nohighlight">\(\text{Ber}(p)\)</span>, is a random variable that takes value 1 with probability <span class="math notranslate nohighlight">\(p\)</span>, and 0 else. Thus, our model can look like:</p>
<div class="math notranslate nohighlight">
\[ \text{Defect Incident, $D_i$} \sim \text{Ber}( \;p(t_i)\; ), \;\; i=1..N\]</div>
<p>where <span class="math notranslate nohighlight">\(p(t)\)</span> is our logistic function and <span class="math notranslate nohighlight">\(t_i\)</span> are the temperatures we have observations about. Notice in the above code we had to set the values of <code class="docutils literal notranslate"><span class="pre">beta</span></code> and <code class="docutils literal notranslate"><span class="pre">alpha</span></code> to 0. The reason for this is that if <code class="docutils literal notranslate"><span class="pre">beta</span></code> and <code class="docutils literal notranslate"><span class="pre">alpha</span></code> are very large, they make <code class="docutils literal notranslate"><span class="pre">p</span></code> equal to 1 or 0. Unfortunately, <code class="docutils literal notranslate"><span class="pre">pm.Bernoulli</span></code> does not like probabilities of exactly 0 or 1, though they are mathematically well-defined probabilities. So by setting the coefficient values to <code class="docutils literal notranslate"><span class="pre">0</span></code>, we set the variable <code class="docutils literal notranslate"><span class="pre">p</span></code> to be a reasonable starting value. This has no effect on our results, nor does it mean we are including any additional information in our prior. It is simply a computational caveat in PyMC3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># connect the probabilities in `p` with our observations through a</span>
<span class="c1"># Bernoulli random variable.</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;bernoulli_obs&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">D</span><span class="p">)</span>
    
    <span class="c1"># Mysterious code to be explained in Chapter 3</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">find_MAP</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">()</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">120000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">)</span>
    <span class="n">burned_trace</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="mi">100000</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> [-------100%-------] 120000 of 120000 in 16.5 sec. | SPS: 7283.1 | ETA: 0.0
</pre></div>
</div>
</div>
</div>
<p>We have trained our model on the observed data, now we can sample values from the posterior. Let’s look at the posterior distributions for <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_samples</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># best to make them 1d</span>
<span class="n">beta_samples</span> <span class="o">=</span> <span class="n">burned_trace</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="c1">#histogram of the samples:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Posterior distributions of the variables $\alpha, \beta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">beta_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;posterior of $\beta$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7A68A6&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">alpha_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;posterior of $\alpha$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#A60628&quot;</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_109_0.png" src="../_images/Ch2_MorePyMC_PyMC3_109_0.png" />
</div>
</div>
<p>All samples of <span class="math notranslate nohighlight">\(\beta\)</span> are greater than 0. If instead the posterior was centered around 0, we may suspect that <span class="math notranslate nohighlight">\(\beta = 0\)</span>, implying that temperature has no effect on the probability of defect.</p>
<p>Similarly, all <span class="math notranslate nohighlight">\(\alpha\)</span> posterior values are negative and far away from 0, implying that it is correct to believe that <span class="math notranslate nohighlight">\(\alpha\)</span> is significantly less than 0.</p>
<p>Regarding the spread of the data, we are very uncertain about what the true parameters might be (though considering the low sample size and the large overlap of defects-to-nondefects this behaviour is perhaps expected).</p>
<p>Next, let’s look at the <em>expected probability</em> for a specific value of the temperature. That is, we average over all samples from the posterior to get a likely value for <span class="math notranslate nohighlight">\(p(t_i)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">temperature</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">5</span><span class="p">,</span> <span class="n">temperature</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">p_t</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">beta_samples</span><span class="p">,</span> <span class="n">alpha_samples</span><span class="p">)</span>

<span class="n">mean_prob_t</span> <span class="o">=</span> <span class="n">p_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mean_prob_t</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;average posterior </span><span class="se">\n</span><span class="s2">probability </span><span class="se">\</span>
<span class="s2">of defect&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;realization from posterior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p_t</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;realization from posterior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior expected value of probability of defect; </span><span class="se">\</span>
<span class="s2">plus realizations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_112_0.png" src="../_images/Ch2_MorePyMC_PyMC3_112_0.png" />
</div>
</div>
<p>Above we also plotted two possible realizations of what the actual underlying system might be. Both are equally likely as any other draw. The blue line is what occurs when we average all the 20000 possible dotted lines together.</p>
<p>An interesting question to ask is for what temperatures are we most uncertain about the defect-probability? Below we plot the expected value line <strong>and</strong> the associated 95% intervals for each temperature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats.mstats</span> <span class="kn">import</span> <span class="n">mquantiles</span>

<span class="c1"># vectorized bottom and top 2.5% quantiles for &quot;confidence interval&quot;</span>
<span class="n">qs</span> <span class="o">=</span> <span class="n">mquantiles</span><span class="p">(</span><span class="n">p_t</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="o">*</span><span class="n">qs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7A68A6&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">qs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% CI&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7A68A6&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mean_prob_t</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;average posterior </span><span class="se">\n</span><span class="s2">probability of defect&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;temp, $t$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;probability estimate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior probability estimates given temp. $t$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_114_0.png" src="../_images/Ch2_MorePyMC_PyMC3_114_0.png" />
</div>
</div>
<p>The <em>95% credible interval</em>, or 95% CI, painted in purple, represents the interval, for each temperature, that contains 95% of the distribution. For example, at 65 degrees, we can be 95% sure that the probability of defect lies between 0.25 and 0.75.</p>
<p>More generally, we can see that as the temperature nears 60 degrees, the CI’s spread out over [0,1] quickly. As we pass 70 degrees, the CI’s tighten again. This can give us insight about how to proceed next: we should probably test more O-rings around 60-65 temperature to get a better estimate of probabilities in that range. Similarly, when reporting to scientists your estimates, you should be very cautious about simply telling them the expected probability, as we can see this does not reflect how <em>wide</em> the posterior distribution is.</p>
</div>
<div class="section" id="what-about-the-day-of-the-challenger-disaster">
<h3>What about the day of the Challenger disaster?<a class="headerlink" href="#what-about-the-day-of-the-challenger-disaster" title="Permalink to this headline">¶</a></h3>
<p>On the day of the Challenger disaster, the outside temperature was 31 degrees Fahrenheit. What is the posterior distribution of a defect occurring,  given this temperature? The distribution is plotted below. It looks almost guaranteed that the Challenger was going to be subject to defective O-rings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>

<span class="n">prob_31</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="mi">31</span><span class="p">,</span> <span class="n">beta_samples</span><span class="p">,</span> <span class="n">alpha_samples</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.995</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">prob_31</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior distribution of probability of defect, given $t = 31$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;probability of defect occurring in O-ring&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_117_0.png" src="../_images/Ch2_MorePyMC_PyMC3_117_0.png" />
</div>
</div>
</div>
<div class="section" id="is-our-model-appropriate">
<h3>Is our model appropriate?<a class="headerlink" href="#is-our-model-appropriate" title="Permalink to this headline">¶</a></h3>
<p>The skeptical reader will say “You deliberately chose the logistic function for <span class="math notranslate nohighlight">\(p(t)\)</span> and the specific priors. Perhaps other functions or priors will give different results. How do I know I have chosen a good model?” This is absolutely true. To consider an extreme situation, what if I had chosen the function <span class="math notranslate nohighlight">\(p(t) = 1,\; \forall t\)</span>, which guarantees a defect always occurring: I would have again predicted disaster on January 28th. Yet this is clearly a poorly chosen model. On the other hand, if I did choose the logistic function for <span class="math notranslate nohighlight">\(p(t)\)</span>, but specified all my priors to be very tight around 0, likely we would have very different posterior distributions. How do we know our model is an expression of the data? This encourages us to measure the model’s <strong>goodness of fit</strong>.</p>
<p>We can think: <em>how can we test whether our model is a bad fit?</em> An idea is to compare observed data (which if we recall is a <em>fixed</em> stochastic variable) with artificial dataset which we can simulate. The rationale is that if the simulated dataset does not appear similar, statistically, to the observed dataset, then likely our model is not accurately represented the observed data.</p>
<p>Previously in this Chapter, we simulated artificial dataset for the SMS example. To do this, we sampled values from the priors. We saw how varied the resulting datasets looked like, and rarely did they mimic our observed dataset. In the current example,  we should sample from the <em>posterior</em> distributions to create <em>very plausible datasets</em>. Luckily, our Bayesian framework makes this very easy. We only need to create a new <code class="docutils literal notranslate"><span class="pre">Stochastic</span></code> variable, that is exactly the same as our variable that stored the observations, but minus the observations themselves. If you recall, our <code class="docutils literal notranslate"><span class="pre">Stochastic</span></code> variable that stored our observed data was:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>observed = pm.Bernoulli(&quot;bernoulli_obs&quot;, p, observed=D)
</pre></div>
</div>
<p>Hence we create:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>simulated_data = pm.Bernoulli(&quot;simulation_data&quot;, p)
</pre></div>
</div>
<p>Let’s simulate 10 000:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">tt</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">temperature</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)))</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;bernoulli_obs&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">D</span><span class="p">)</span>
    
    <span class="n">simulated</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;bernoulli_sim&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">test_value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">(</span><span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Assigned BinaryGibbsMetropolis to bernoulli_sim
 [-------100%-------] 10000 of 10000 in 27.8 sec. | SPS: 359.1 | ETA: 0.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">simulations</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="s2">&quot;bernoulli_sim&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">simulations</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Simulated dataset using posterior parameters&quot;</span><span class="p">)</span>
<span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">simulations</span><span class="p">[</span><span class="mi">1000</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10000, 23)
</pre></div>
</div>
<img alt="../_images/Ch2_MorePyMC_PyMC3_120_1.png" src="../_images/Ch2_MorePyMC_PyMC3_120_1.png" />
</div>
</div>
<p>Note that the above plots are different (if you can think of a cleaner way to present this, please send a pull request and answer <a class="reference external" href="http://stats.stackexchange.com/questions/53078/how-to-visualize-bayesian-goodness-of-fit-for-logistic-regression">here</a>!).</p>
<p>We wish to assess how good our model is. “Good” is a subjective term of course, so results must be relative to other models.</p>
<p>We will be doing this graphically as well, which may seem like an even less objective method. The alternative is to use <em>Bayesian p-values</em>. These are still subjective, as the proper cutoff between good and bad is arbitrary. Gelman emphasises that the graphical tests are more illuminating [7] than p-value tests. We agree.</p>
<p>The following graphical test is a novel data-viz approach to logistic regression. The plots are called <em>separation plots</em>[8]. For a suite of models we wish to compare, each model is plotted on an individual separation plot. I leave most of the technical details about separation plots to the very accessible <a class="reference external" href="http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf">original paper</a>, but I’ll summarize their use here.</p>
<p>For each model, we calculate the proportion of times the posterior simulation proposed a value of 1 for a particular temperature, i.e. compute <span class="math notranslate nohighlight">\(P( \;\text{Defect} = 1 | t, \alpha, \beta )\)</span> by averaging. This gives us the posterior probability of a defect at each data point in our dataset. For example, for the model we used above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_probability</span> <span class="o">=</span> <span class="n">simulations</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;posterior prob of defect | realized defect &quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">D</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.2f</span><span class="s2">                     |   </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">posterior_probability</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>posterior prob of defect | realized defect 
0.40                     |   0
0.25                     |   1
0.28                     |   0
0.32                     |   0
0.36                     |   0
0.19                     |   0
0.17                     |   0
0.25                     |   0
0.73                     |   1
0.53                     |   1
0.25                     |   1
0.10                     |   0
0.36                     |   0
0.80                     |   1
0.36                     |   0
0.13                     |   0
0.25                     |   0
0.07                     |   0
0.12                     |   0
0.09                     |   0
0.13                     |   1
0.12                     |   0
0.71                     |   1
</pre></div>
</div>
</div>
</div>
<p>Next we sort each column by the posterior probabilities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">posterior_probability</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;probb | defect &quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">D</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.2f</span><span class="s2">  |   </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">posterior_probability</span><span class="p">[</span><span class="n">ix</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">D</span><span class="p">[</span><span class="n">ix</span><span class="p">[</span><span class="n">i</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>probb | defect 
0.07  |   0
0.09  |   0
0.10  |   0
0.12  |   0
0.12  |   0
0.13  |   1
0.13  |   0
0.17  |   0
0.19  |   0
0.25  |   1
0.25  |   0
0.25  |   1
0.25  |   0
0.28  |   0
0.32  |   0
0.36  |   0
0.36  |   0
0.36  |   0
0.40  |   0
0.53  |   1
0.71  |   1
0.73  |   1
0.80  |   1
</pre></div>
</div>
</div>
</div>
<p>We can present the above data better in a figure: I’ve wrapped this up into a <code class="docutils literal notranslate"><span class="pre">separation_plot</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">separation_plot</span> <span class="kn">import</span> <span class="n">separation_plot</span>


<span class="n">figsize</span><span class="p">(</span><span class="mf">11.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">separation_plot</span><span class="p">(</span><span class="n">posterior_probability</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_126_0.png" src="../_images/Ch2_MorePyMC_PyMC3_126_0.png" />
</div>
</div>
<p>The snaking-line is the sorted probabilities, blue bars denote defects, and empty space (or grey bars for the optimistic readers) denote non-defects.  As the probability rises, we see more and more defects occur. On the right hand side, the plot suggests that as the posterior probability is large (line close to 1), then more defects are realized. This is good behaviour. Ideally, all the blue bars <em>should</em> be close to the right-hand side, and deviations from this reflect missed predictions.</p>
<p>The black vertical line is the expected number of defects we should observe, given this model. This allows the user to see how the total number of events predicted by the model compares to the actual number of events in the data.</p>
<p>It is much more informative to compare this to separation plots for other models. Below we compare our model (top) versus three others:</p>
<ol class="simple">
<li><p>the perfect model, which predicts the posterior probability to be equal 1 if a defect did occur.</p></li>
<li><p>a completely random model, which predicts random probabilities regardless of temperature.</p></li>
<li><p>a constant model:  where <span class="math notranslate nohighlight">\(P(D = 1 \; | \; t) = c, \;\; \forall t\)</span>. The best choice for <span class="math notranslate nohighlight">\(c\)</span> is the observed frequency of defects, in this case 7/23.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figsize</span><span class="p">(</span><span class="mf">11.</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span>

<span class="c1"># Our temperature-dependent model</span>
<span class="n">separation_plot</span><span class="p">(</span><span class="n">posterior_probability</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Temperature-dependent model&quot;</span><span class="p">)</span>

<span class="c1"># Perfect model</span>
<span class="c1"># i.e. the probability of defect is equal to if a defect occurred or not.</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">D</span>
<span class="n">separation_plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Perfect model&quot;</span><span class="p">)</span>

<span class="c1"># random predictions</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
<span class="n">separation_plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random model&quot;</span><span class="p">)</span>

<span class="c1"># constant model</span>
<span class="n">constant_prob</span> <span class="o">=</span> <span class="mf">7.</span><span class="o">/</span><span class="mi">23</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
<span class="n">separation_plot</span><span class="p">(</span><span class="n">constant_prob</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Constant-prediction model&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_128_0.png" src="../_images/Ch2_MorePyMC_PyMC3_128_0.png" />
<img alt="../_images/Ch2_MorePyMC_PyMC3_128_1.png" src="../_images/Ch2_MorePyMC_PyMC3_128_1.png" />
<img alt="../_images/Ch2_MorePyMC_PyMC3_128_2.png" src="../_images/Ch2_MorePyMC_PyMC3_128_2.png" />
<img alt="../_images/Ch2_MorePyMC_PyMC3_128_3.png" src="../_images/Ch2_MorePyMC_PyMC3_128_3.png" />
</div>
</div>
<p>In the random model, we can see that as the probability increases there is no clustering of defects to the right-hand side. Similarly for the constant model.</p>
<p>The perfect model, the probability line is not well shown, as it is stuck to the bottom and top of the figure. Of course the perfect model is only for demonstration, and we cannot infer any scientific inference from it.</p>
<div class="section" id="exercises">
<h4>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h4>
<p>1. Try putting in extreme values for our observations in the cheating example. What happens if we observe 25 affirmative responses? 10? 50?</p>
<p>2. Try plotting <span class="math notranslate nohighlight">\(\alpha\)</span> samples versus <span class="math notranslate nohighlight">\(\beta\)</span> samples.  Why might the resulting plot look like this?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#type your code here.</span>
<span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">alpha_samples</span><span class="p">,</span> <span class="n">beta_samples</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Why does the plot look like this?&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Ch2_MorePyMC_PyMC3_132_0.png" src="../_images/Ch2_MorePyMC_PyMC3_132_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>[1] Dalal, Fowlkes and Hoadley (1989),JASA, 84, 945-957.</p></li>
<li><p>[2] German Rodriguez. Datasets. In WWS509. Retrieved 30/01/2013, from <a class="reference external" href="http://data.princeton.edu/wws509/datasets/#smoking">http://data.princeton.edu/wws509/datasets/#smoking</a>.</p></li>
<li><p>[3] McLeish, Don, and Cyntha Struthers. STATISTICS 450/850 Estimation and Hypothesis Testing. Winter 2012. Waterloo, Ontario: 2012. Print.</p></li>
<li><p>[4] Fonnesbeck, Christopher. “Building Models.” PyMC-Devs. N.p., n.d. Web. 26 Feb 2013. <a class="reference external" href="http://pymc-devs.github.com/pymc/modelbuilding.html">http://pymc-devs.github.com/pymc/modelbuilding.html</a>.</p></li>
<li><p>[5] Cronin, Beau. “Why Probabilistic Programming Matters.” 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013. <a class="reference external" href="https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1">https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1</a>.</p></li>
<li><p>[6] S.P. Brooks, E.A. Catchpole, and B.J.T. Morgan. Bayesian animal survival estimation. Statistical Science, 15: 357–376, 2000</p></li>
<li><p>[7] Gelman, Andrew. “Philosophy and the practice of Bayesian statistics.” British Journal of Mathematical and Statistical Psychology. (2012): n. page. Web. 2 Apr. 2013.</p></li>
<li><p>[8] Greenhill, Brian, Michael D. Ward, and Audrey Sacks. “The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.” American Journal of Political Science. 55.No.4 (2011): n. page. Web. 2 Apr. 2013.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">HTML</span>


<span class="k">def</span> <span class="nf">css_styling</span><span class="p">():</span>
    <span class="n">styles</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;../styles/custom.css&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">HTML</span><span class="p">(</span><span class="n">styles</span><span class="p">)</span>
<span class="n">css_styling</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
    @font-face {
        font-family: "Computer Modern";
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');
    }
    @font-face {
        font-family: "Computer Modern";
        font-weight: bold;
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');
    }
    @font-face {
        font-family: "Computer Modern";
        font-style: oblique;
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');
    }
    @font-face {
        font-family: "Computer Modern";
        font-weight: bold;
        font-style: oblique;
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');
    }
    div.cell{
        width:800px;
        margin-left:16% !important;
        margin-right:auto;
    }
    h1 {
        font-family: Helvetica, serif;
    }
    h4{
        margin-top:12px;
        margin-bottom: 3px;
       }
    div.text_cell_render{
        font-family: Computer Modern, "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;
        line-height: 145%;
        font-size: 130%;
        width:800px;
        margin-left:auto;
        margin-right:auto;
    }
    .CodeMirror{
            font-family: "Source Code Pro", source-code-pro,Consolas, monospace;
    }
    .prompt{
        display: None;
    }
    .text_cell_render h5 {
        font-weight: 300;
        font-size: 22pt;
        color: #4057A1;
        font-style: italic;
        margin-bottom: .5em;
        margin-top: 0.5em;
        display: block;
    }

    .warning{
        color: rgb( 240, 20, 20 )
        }  
</style>
<script>
    MathJax.Hub.Config({
                        TeX: {
                           extensions: ["AMSmath.js"]
                           },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
                },
                displayAlign: 'center', // Change this to 'center' to center equations.
                "HTML-CSS": {
                    styles: {'.MathJax_Display': {"margin": 4}}
                }
        });
</script>
</div></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="Ch1_Introduction_PyMC3.html" title="previous page">Probabilistic Programming</a>
    <a class='right-next' id="next-link" href="Ch3_IntroMCMC_PyMC3.html" title="next page">Chapter 3</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By CamDavidsonPilon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>